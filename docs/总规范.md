# 总规范

## 总览

CDNN的根基在于Tensor类，它是整个框架的核心，任何类都离不开它。

自顶向下来说，CDNN的神经网络是Module，Module是由一个Loss、一个Optim、许多个Layer组成的，它具有一个Forward方法能够利用输入计算到输出，也具有Backward方法进行参数更新。在创建一个神经网络时，需要先创建一个Loss和Optim，再将网络的层级一层一层地插入进去，随后就可以开始训练了。

Dataset和DataLoader是独立于Module而存在的，Dataset需要使用者来编写，它需要包含len，get，free三个方法。DataLoader不需要使用者编写，它只需要接受一个Dataset即可完成初始化。

## Tensor

Tensor具有如下关键要素：
- 任何数据在Tensor内部均以连续内存的一维数组的形式存储，数组类型是double
- Tensor会保留一个形状数组用于指示形状（stride数组方便计算，本质也是形状数组）
- 除以上两点之外Tensor不保留其他任何信息

Tensor具有许多方法来进行操作。关于Tensor操作函数，除了哪些在函数名中标出了Implace的函数，其他函数都会创建一个新的Tensor，而且不会释放老的Tensor。

## Loss

Loss是以类的形式而提供的，但是它除了类型id，类内不保存任何成员变量。它只是提供两个方法：
- loss方法，利用两个形状相同的Tensor（一个输出值，一个标签值），计算损失值（并不要求这两个Tensor都是一维），返回值是一个double
- grad_loss方法，利用两个形状相同的Tensor（一个输出值，一个标签值），计算Loss对输出值向量的梯度，返回值是一个与输出值形状相同的Tensor

Loss函数并不拥有任何对象，传入的Tensor所有权仍然归调用者，grad_loss返回值的所有权也归调用者。

Loss对象应该被使用者创建，随后被传入Module，此后所有权一直归Module所有，Module释放时会自动释放Loss对象。

## Optim

Optim是以类的形式而提供的，它自创建开始就保存着一些创建参数，如学习率等。Optim需要加入到Module中才能发挥作用。使用者应创建Optim对象，随后将其传入Module，此后Optim的所有权一直归Module所有，Module释放时会自动释放Loss对象。

Optim内部保存了Module所有层的参数以及参数梯度的信息，但它并不拥有它们，只是能够读取与修改。

Optim拥有的唯一的成员函数是step函数，编写Optim时只需要关注这个step函数即可。step函数利用参数梯度进行所有层参数的更新。

Optim内部还具有void*指针，用于存放一些其他创建参数，需要注意的是为了保证dump的正确性，这个void指针指向的量不应该是或含有指针变量的结构体。

## Layer

Layer是神经网络的主要组成部分，它以类的形式给出，包含以下元素：
- Layer类别号
- 输入大小，即输入Tensor的总元素个数，Tensor的形状则无关紧要
- 输出大小，即输出Tensor的总元素个数，Tensor的形状则无关紧要
- 参数，必须是一个Tensor，可以是空指针。
- 上次输入，在每次调用forward之后，Layer会获得传入参数的所有权，将其保存起来。在下一次调用forward时将其释放。从未调用forward时，这个指针是空的。
- 上次输出，在每次调用forward之后，Layer会将输出复制一份保存在Layer内部。在下一次调用forward时将其释放。从未调用forward时，这个指针是空的。
- 参数梯度，与参数的形状相同，在每次调用backward之后会更新，可以是空指针。

Layer包含forward和backward这两个方法，这是Layer的核心。

forward方法接收一个大小与input_size相同的Tensor（Tensor的形状会在forward内部进行检查），然后利用参数进行一些计算，并返回输出Tensor。在每次调用forward时，Layer会将输入的Tensor的所有权占为己有（意思是调用者就不应该继续使用输入Tensor了），并将输出的Tensor复制一份保存起来。

backward方法接收一个大小与output_size相同的Tensor（Tensor的形状会在backward内部进行检查），这个Tensor的含义是Loss对输出Tensor的梯度。backward将会更新参数梯度，并计算Loss对输入Tensor的梯度，将其返回。在backward中，Layer不会修改传入参数的所有权，也不会复制任何Tensor，对输出而言，它也只是创建一个新的Tensor，填值并返回。

## Module

Module是神经网络主类，它管理Loss、Optim以及所有的Layers，同时它还保存着上一次调用forward之后的输出。

它的使用规范如下步骤所示：
- 初始化Module
- 传入Loss与Optim
- 使用add_node构建网络
- 使用attach将Optim附加在所有层上
- 开始循环，每个循环中使用forward获取结果，再使用backward更新参数梯度，最后使用step来更新参数。

对于Module而言，它需要在自己被释放时将传入的Loss、Optim与Layers全部释放。

## Layer字典

### ReLU

一种没有参数的激活函数

- 没有参数
- 输入与输出形状相同
- forward：对输入Tensor的所有值依次判断，如果大于0就保持原状，如果小于等于0就归0
- backward对参数：没有参数
- backward对输入：对输入Tensor的所有值依次判断，如果大于0就在对应位置上置1，如果小于等于0就置0

### Linear

全连接层，赋予模型线性拟合的能力

- 参数：权重矩阵W和偏置b

全连接层不会关心输入Tensor的形状，它会将输入Tensor的最后一维视为样本维，并将其他维合为一维，这样经过处理后的输入Tensor形状就变成了（K，N），其中N是行，是样本总数。输出的Tensor形状是（M，N），而K应该与输入大小相等，M应该与输出大小相等。在调用方法时，线性层会进行如上所述的形状检查。

权重矩阵W的形状是（K，M），偏置b的形状是（K，1）

- 输入形状是（K1，K2，...，Kn，N），其中Sigma Ki等于输入大小K。输出形状是（M，N）
- forward：对输入Tensor I重整形状为（K，N），计算IW+b